[{"uri":"https://vdhxi.github.io/5-workshop/5.3-compute/5.3.1-ec2/","title":"Amazon EC2","tags":[],"description":"","content":"Initialize EC2 Instance Steps 1. Create Security Group Create a Security Group allowing HTTP (80), HTTPS (443), SSH (22), and Custom TCP (8080 - Spring Boot port). 2. Launch Instance Access EC2 Dashboard -\u0026gt; Launch Instances.\nName: Auction-Backend.\nSelect OS: Amazon Linux 2023 or Ubuntu. Select Instance Type: t3.medium (as proposed).\nSelect Key Pair (create new if not exists).\nNetwork settings: Select VPC, Public Subnet, and the created Security Group. Configure storage (default 8GB or increase if needed).\nAdvanced details:\nIAM instance profile: Select Auction-EC2-Role. Click Launch instance.\n3. Assign Elastic IP (Optional) If you want a fixed Public IP.\nGo to Elastic IPs -\u0026gt; Allocate Elastic IP address. Select the created IP -\u0026gt; Associate Elastic IP address. Select the created Instance. "},{"uri":"https://vdhxi.github.io/5-workshop/5.2-database-storage/5.2.1-rds/","title":"Amazon RDS","tags":[],"description":"","content":"Initialize Amazon RDS (MySQL) We will use Amazon RDS MySQL to store the main data of the system.\nSteps 1. Create Security Group for RDS First, we need to create a Security Group allowing connection on port 3306 from EC2. 2. Create Subnet Group Go to RDS Dashboard -\u0026gt; Subnet groups -\u0026gt; Create DB subnet group. Select the VPC and the created Private Subnets. 3. Create Database Select Databases -\u0026gt; Create database.\nSelect Standard create -\u0026gt; MySQL.\nSelect version (Engine Version). Select Free tier (if using a new account) or Dev/Test. Set DB instance identifier, Master username and password. Select Instance class (e.g., db.t3.micro).\nConfigure Storage. Configure Connectivity: Select VPC, Subnet Group, and the created Security Group. Configure authentication (Password authentication). Click Create database. "},{"uri":"https://vdhxi.github.io/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Performance benefits of the new memory-optimized Amazon EC2 R8a instances We recently announced the general availability of Amazon EC2 R8a instances — the latest addition to our memory-optimized instance family powered by AMD CPUs. These instances feature 5th Generation AMD EPYC processors (code-named “Turin”) with a maximum frequency of up to 4.5 GHz. In this post, I’ll put these instances to the test and run MySQL benchmarks — but first, let’s look at some important characteristics you should know about them.\nKey features of R8a instances Each vCPU on a R8a instance corresponds to a physical CPU core (a practice AWS began with the 7th generation AMD instances). This means there is no Simultaneous Multithreading (SMT). Each vCPU is assigned its own physical core — delivering more stable and consistent performance by avoiding resource sharing or contention between threads; this is particularly important for performance-sensitive or latency-sensitive workloads.\nWhen evaluating and migrating to R8a, you should re-evaluate your CPU utilization thresholds — as you can likely utilize more CPU per instance without impacting your workload\u0026rsquo;s SLA (Service Level Agreement).\nR8a supports a very wide range of configurations — up to 192 vCPUs with 1,536 GiB of RAM. Below is a detailed table of common sizes:\nInstance size vCPU Memory (GiB) Instance storage Network bandwidth (Gbps) EBS bandwidth (Gbps) r8a.medium 1 8 EBS Only Up to 12.5 Up to 10 r8a.large 2 16 EBS Only Up to 12.5 Up to 10 r8a.xlarge 4 32 EBS Only Up to 12.5 Up to 10 r8a.2xlarge 8 64 EBS Only Up to 15 Up to 10 r8a.4xlarge 16 128 EBS Only Up to 15 Up to 10 r8a.8xlarge 32 256 EBS Only 15 10 r8a.12xlarge 48 384 EBS Only 22.5 15 r8a.16xlarge 64 512 EBS Only 30 20 r8a.24xlarge 96 768 EBS Only 40 30 r8a.48xlarge 192 1536 EBS Only 75 60 r8a.metal-24xl 96 768 EBS Only 40 30 r8a.metal-48xl 192 1536 EBS Only 75 60 MySQL Performance Testing with HammerDB R8a instances are an excellent choice for MySQL databases, so I believe this is a fitting context to illustrate their capabilities. To test MySQL, I used a set of scripts developed by my colleagues to track MySQL performance across various software versions and EC2 instance types. These scripts are hosted in the repo-collection repository, an open-source, scalable framework designed for performance testing based on real-world workloads rather than micro-benchmarks. This framework was built to provide a performance measurement reference standard usable across organizations, currently focused primarily on MySQL and actively used in discussions with Linux Kernel developers and maintainers. Additionally, it supports tracking any performance impacts arising from MySQL source code changes. The scripts in this repository handle setting up a MySQL database for testing, along with a load generator running the HammerDB benchmark.\nFor this benchmark, I used a r6a.24xlarge instance as the load generator, and r6a.xlarge, r7a.xlarge, and r8a.xlarge instances as the MySQL database servers, all deployed within the same AWS Availability Zone (AZ). I chose a single-AZ configuration to minimize latency variability that could arise from traffic crossing multiple AZs. This is not a production simulation configuration, and I highly recommend using multiple AZs for production workloads. Each MySQL instance was tested independently using the same HammerDB load generator. Each test was run three times, and the results were averaged over the three runs. The architecture diagram is illustrated in the following figure:\nOverall HammerDB Results R8a instances demonstrated superior performance in the HammerDB benchmark for MySQL databases. In the HammerDB overall score category, R8a instances scored 55% higher than R7a and 74% higher than R6a.\nHammerDB Transactions Per Minute (TPM) Testing R8a instances also showed significant improvement in this category. Compared to the previous generation R7a instances, R8a delivered 32% higher performance. When compared to R6a instances, R8a achieved an improvement of up to 63%.\nHammerDB P99 Latency Results R8a instances showed a clear improvement in P99 latency, reflecting the efficiency of the 5th Generation AMD EPYC processors along with higher memory bandwidth. R8a achieved a 14% reduction in latency compared to R7a and a 25% reduction compared to R6a.\nConclusion The R8a instance, built on the AWS Nitro System with sixth-generation Nitro Cards, is the ideal choice for high-performance, memory-intensive workloads — such as SQL/NoSQL databases, distributed web-scale in-memory caches, in-memory databases, real-time big data analytics, and EDA (Electronic Design Automation) applications. With 12 different sizes (including 2 bare-metal sizes), R8a is suitable for everything from small applications to large-scale systems. R8a is SAP certified and offers 38% more SAPS than R7a. If you are currently using R6a (6th generation) instances, you should migrate to R8a to take advantage of the clear price-performance benefits. Maintaining modern infrastructure also helps reduce operational costs and delivers more features to customers.\n"},{"uri":"https://vdhxi.github.io/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Optimize latency-sensitive workloads with Amazon EC2 detailed NVMe statistics Amazon Elastic Cloud Compute (Amazon EC2) instances with locally attached NVMe storage can provide the performance needed for workloads demanding ultra-low latency and high I/O throughput. High-performance workloads, from high-frequency trading applications and in-memory databases to real-time analytics engines and AI/ML inference, need comprehensive performance tracking. Operating system tools like iostat and sar provide valuable system-level insights, and Amazon CloudWatch offers important disk IOPs and throughput measurements, but high-performance workloads can benefit from even more detailed visibility into instance store performance.\nFor latency-sensitive applications where every millisecond counts, enhanced performance monitoring tools provide deep visibility into storage systems, so your teams can track and analyze behavior at a 1 second granularity. This detailed insight can help your organization detect bottlenecks quickly, fine-tune application performance, and deliver reliable service.\nIn this post, we discuss how you can use Amazon EC2 detailed performance statistics for instance store NVMe volumes, a set of new metrics that provide per-second granularity, to provide real-time visibility into your locally attached storage performance. These statistics are similar to the Amazon EBS detailed performance statistics, providing a consistent monitoring experience across both storage types. You can access these statistics directly from your NVMe devices attached to the Amazon EC2 instance using nvme-cli or using CloudWatch agent to monitor I/O performance at the storage level. We also provide examples of how to use these statistics to identify performance bottlenecks.\nFeature overview Amazon EC2 Nitro-based instances with locally attached NVMe instance storage now offer 11 comprehensive metrics at per-second granularity. These metrics, similar to EBS volume metrics, include queue length measurements, IOPS, throughput data, and IO latency histograms for the locally attached NVMe instance storage. Additionally, they also include IO size-specific latency histograms to provide even more detailed insights into performance patterns of the local NVMe instance storage. These metrics are collected and presented separately for each individual NVMe volume available on an instance.\nThe statistics are presented in three main formats:\nThe statistics are presented in three main formats: Real-time queue length, displaying the current value at the time of your query Latency histograms visualizing the distribution of IO operations across different latency ranges by displaying both cumulative view and IO size-specific distributions Prerequisites To access detailed performance statistics for local instance storage, complete the following steps:\nLaunch a new Amazon EC2 Nitro instance or use an existing one, then connect to it using SSH or your preferred connection method. Identify the NVMe device associated with the local storage to query for the performance statistics. For example, you can run the nvme-cli command in the CLI to output all NVMe devices on the instance. bash $ sudo nvme list. The following is an example output of the list command that lists the NVMe devices on the instance and their volume Serial Numbers (SN; masked in the below output for privacy). In this demonstration, consider that the local storage used by your application is /dev/nvme1n1.\nIf you are using Amazon Linux 2023 version 2023.8.20250915 (or later) or Amazon Linux 2 2.0.20251014.0 (or later) you can proceed to Step 4 because nvme-cli will use the latest version. If you are using an earlier Amazon Linux version, update the nvme-cli using the following command, where 2023.8.20250915 can be replaced with the latest Amazon Linux 2023 version: bash $ sudo dnf upgrade --releasever=2023.8.20250915 Run the nvme-cli, with the correct permissions, and pass the device as a parameter. You can use --help to get details on the command usage: bash $ sudo nvme amzn stats --help Example output:\nIf you prefer output in a JSON format, you can provide the -o json parameter to the command.\nbash $ sudo nvme amzn stats /dev/nvme1n1 -o json The following output (without the -o json parameter) shows cumulative read/write operations, read/write bytes, total processing time (read and write in microseconds), and duration (in microseconds) when application attempted to exceed the instance’s IOPS/throughput limits.\nIt also displays read/write I/O latency histograms, with each row representing completed I/O operations within a specific bin of time (in microseconds).\nIf you want to view the latency histograms across 5 different IO bands: (0, 512 Byte], (512B, 4KiB], (4KiB, 8KiB], (8KiB 32KiB], (32 KiB, MAX], you can provide --details or -d parameter to the command:\nbash $ sudo nvme amzn stats -d /dev/nvme1n The following image is an excerpt of the above command’s output, showing the additional latency histograms (read and write) of the 5 different IO bands.\nYou can run the stats command at a per second granularity. You can also write scripts to pull the stats at a desired interval (every second or any other duration) with each subsequent output reflecting the updated cumulative totals for the metrics. Calculating the difference in the statistics across the last two outputs allows you to derive insight into the instance storage profile during the interval. Below is a sample script you can use to pull the stats at a default interval of 1 second or at your desired interval.\n#!/bin/bash # interval of 1 second INTERVAL=${1:-1} while true; do echo \u0026#34;=== $(date) ===\u0026#34; sudo nvme amzn stats /dev/nvme1 || break echo sleep $INTERVAL done You can save this script, make it executable and run it at either the default 1-second interval or provide a custom interval when executing the script. For example, if you saved the script as nvme_stats.sh, you could use the following commands to make it executable and run to get the output at the default 1-second interval (assuming you are in the same directory as that of nvme_stats.sh).\nchmod +x nvme_stats.sh ./nvme_stats.sh If, for instance, you want to get the output at every 5 seconds, you can use the command below (after making the script executable)\n./nvme_stats.sh 5 You can also integrate with CloudWatch using CloudWatch agent to collect and publish these statistics for historical tracking, trend visualization through dashboards, and performance-based alerts to correlate with application metrics and automated notifications for performance issues.\nDeriving insights from the Amazon EC2 instance store NVMe detailed performance statistics Similar to EBS detailed performance statistics, you can use Amazon EC2 instance store NVMe statistics to troubleshoot various workload performance issues. As mentioned in the preceding section, you can also use the detailed statistics to view I/O latency histograms to observe the spread of I/O latency within the period. You can use the read/write operations and time spent statistics to calculate the average latency. The detailed statistics show the average latency at per-second granularity.\nThe next two example scenarios demonstrate key performance analysis using the statistics. In Scenario 1, we will use the EC2 Instance Local Storage Performance Exceeded (us) metric to check if I/O demands exceed instance storage capabilities, helping with instance right-sizing for sufficient I/O application performance. In Scenario 2, we will use IO-size specific histograms (using --details) to diagnose how large block writes affect subsequent read performance – an issue typically hidden by traditional monitoring tools’ aggregated metrics across all IO sizes.\nScenario 1: Identifying when applications exceed instance storage performance limits Understanding whether your application’s I/O demands exceed your instance store volumes’ capabilities is important for performance troubleshooting. When applications generate I/O workloads that consistently attempt to exceed the IOPS and throughput limits of specific Amazon EC2 instance types, you’ll experience increased latency and degraded performance. The EC2 Instance Local Storage Performance Exceeded (us) metric helps identify these scenarios by showing the duration (in microseconds) when workloads exceeded supported instance performance. A non-zero value or increasing count between snapshots indicates your current instance size or type may not provide sufficient I/O performance for your application.\nThe following section shows how to identify if an application is sending more IOPS than the instance’s local storage can support.\nThe example scenario: An application on an i3en.xlarge instance shows elevated write latency of \u0026gt;1ms. You want to determine if the application’s workload is exceeding the instance’s NVMe volume supported performance.\nSelect the Instance Storage NVMe device you want to analyze – Identify the instance you want to analyze for the application experiencing elevated latency. Identify the NVMe device – Use the following nvme-cli command, and identify the NVMe device associated with that instance storage. $ sudo nvme list Example scenario: We used the list and identified /dev/nvme1n1 as the NVMe device associated with the i3en.xlarge instance that is running the application which is currently seeing elevated write latency \u0026gt;1ms (while read latency is \u0026lt;50us as per normal conditions), so now we want to. analyze it.\nCollect statistics for the device at a single point in time or at desired intervals – Collect the detailed performance statistics using the nvme-cli command or use the sample script provided in previous section to capture statistics at the desired intervals, if needed. $ sudo nvme amzn stats /dev/nvme1n1 Example scenario: We choose to collect the statistics only once after noticing elevated write latency of the application.\nAnalyze the statistics to check if the application demands more than the supported performance of the instance storage – Confirm existence of overall I/O latency degradation by comparing two sets of read/write I/O latency histograms taken some time apart.Example scenario: The following output shows Read IO histogram of the NVMe local instance storage taken 40 seconds apart with no read IO latency issues (as normal read latency for this workload is \u0026lt; 50 us). Metric captured at time T:\nMetric captured at time T+40s:\nThe following output shows Write IO histogram taken 40 seconds apart. We can discern that many write IOs fall into the 1ms – 2ms latency range, which is not expected for this application.\nMetric captured at time T:\nMetric captured at time T+40s:\nAnalyze the EC2 Instance Local Storage Performance Exceeded (us) metric which shows total time (in microseconds) IOPS requests exceed volume limits. Ideally, the incremental count of this metric between two snapshot times should be minimal, as any value above 0 indicates that the workload demanded more IOPS than the volume could deliver.Example scenario: Comparing metrics 40 seconds apart shows that for more than 34 seconds, the application’s IOPS demands surpassed the IOPS supported by the local instance storage. This explains elevated write latency: excess IOPS above what the underlying storage can physically handle queue the operations, increasing wait times. This indicates that the i3en.xlarge instance chosen to run this application cannot meet the application’s performance requirements, suggesting either upgrading to a larger instance size or re-evaluating the instance type itself. Metric captured at time T:\nMetric captured at time T+40s:\nIt’s important to have the right instance size to avoid performance bottlenecks to your application. Refer to the Amazon EC2 instance documentation for more information on the different instances and their storage size.\nScenario 2: Identifying the block size causing elevated latency in your applications Many storage performance issues arise from complex interactions between read and write operations with different I/O sizes, which traditional system-level monitoring tools like iostat or sar cannot effectively diagnose due to their aggregated metrics across all I/O sizes. EC2 instance store NVMe detailed performance statistics solves this by providing I/O-size specific latency histograms through the --details option in NVMe CLI. These histograms show latency data for different I/O size ranges: (0, 512 Byte], (512B, 4KiB], (4KiB, 8KiB], (8KiB, 32KiB], (32KiB, MAX], for a more precise correlation between application workload patterns and I/O size-specific latency metrics for targeted optimizations.\nIn this example scenario, your application performs small reads (typically \u0026lt;=4KiB, like metadata read) followed by large writes (\u0026gt;=32KiB) and shows unexpectedly high read latency. This common issue occurs when large writes impact subsequent read operations’ performance, creating a cascading effect on overall I/O performance.\nGather read and write IO latency by size ranges – Use the NVMe CLI with the --details option to gather read and write IO latency by size ranges: $ sudo nvme amzn stats /dev/nvme1n1 --details Confirm existence of overall IO latency degradation – In the example scenario, examining overall IO latency, both read (left) and write (right) operations are showing higher than expected latency. Examine the output for patterns across different IO size bands – Analyzing latency by operation sizes shows small read operations (512 bytes to 4K), typically fast, are experiencing unexpected latency spikes while large writes (32K+) show significant delays. Small reads should theoretically maintain good performance regardless of other I/O activities. The observed pattern indicates that the backed-up large write operations create system-wide congestion, affecting all I/O operations of types and sizes. Despite the storage system’s capability to handle small reads efficiently, the queued large writes slow down both read and write operations at the application level.\nBased on this analysis, we can implement several targeted optimizations to the application, like using smaller block sizes for write operations when possible, or batching smaller writes instead of performing large single writes.\nClean up If you created an Amazon EC2 instance with NVMe volume for this exercise, then terminate and delete the appropriate instance to avoid future costs.\nConclusion Amazon EC2 detailed performance statistics for instance store NVMe volumes provide real-time, sub-minute storage performance monitoring, similar to the detailed performance statistics available for Amazon EBS volumes. This offers consistent monitoring experience across both storage types, with additional IO-size based latency histograms for instance storage for better optimization of I/O patterns, and more effective troubleshooting.\nTo learn more about Amazon EC2 instance store NVMe volumes, optimization techniques for latency-sensitive workloads or other Amazon EC2 related topics, visit the Amazon EC2 documentation page or explore our other AWS Storage Blog posts on performance optimization.\nWe’d love to hear how you’re using these statistics to enhance your workloads, or if you have any questions, in the comments section below.\n"},{"uri":"https://vdhxi.github.io/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"How to export to Amazon S3 Tables by using AWS Step Functions Distributed Map Companies running serverless workloads often need to perform extract, transform, and load (ETL) operations on data files stored in Amazon Simple Storage Service (Amazon S3) buckets. Though traditional approaches such as an AWS Lambda trigger for Amazon S3 or Amazon S3 Event Notifications can handle these operations, they might fall short when workflows require enhanced visibility, control, or human intervention. For example, some processes might need manual review of failed records or explicit approval before proceeding to subsequent stages. Customer orchestration solutions to these issues can prove to be complex and error prone. AWS Step Functions address these challenges by providing built-in workflow management and monitoring capabilities. The Step Functions Distributed Map feature is designed for high-throughput, parallel data processing workflows so that companies can handle complex ETL jobs, fan-out processing, and data visualization at scale. Distributed Map handles each dataset item as an independent child workflow, processing millions of records while maintaining built-in concurrency controls, fault tolerance, and progress tracking. The processed data can be seamlessly exported to various destinations, including Amazon S3 Tables with Apache Iceberg support.\nIn this post, we show how to use Step Functions Distributed Map to process Amazon S3 objects and export results to Amazon S3 Tables, creating a scalable and maintainable data processing pipeline.\nSee the associated GitHub repository for detailed instructions about deploying this solution as well as sample code. Solution overview Consider a consumer electronics company that regularly participates in industry trade shows and conferences. During these events, interested attendees fill out paper sign-up forms to request product demos, receive newsletters, or join early access programs. After the events, the company’s team scans hundreds of thousands of these forms and uploads them to Amazon S3.Rather than manually reviewing each form, the company wants to automate the extraction of key customer details such as name, email address, mailing address, and interest areas. They’d like to store this structured data in S3 Tables with Apache Iceberg format for downstream analytics and marketing campaign targeting.\nLet’s look at how this post’s solution uses Distributed Map to process PDFs in parallel, extract data using Amazon Textract, and write the cleaned output directly to S3 Tables. The result is scalable, serverless post-event data onboarding, as shown in the following figure.\nThe data processing workflow as shown in the preceding diagram includes the following steps:\nA user uploads customer interest forms as scanned PDFs to an Amazon S3 bucket. An Amazon EventBridge Scheduler rule triggers at regular intervals, initiating a Step Functions workflow execution. The workflow execution activates a Step Functions Distributed Map state, which lists all PDF files uploaded to Amazon S3 since the previous run. The Distributed Map iterates over the list of objects and passes each object’s metadata (bucket, key, size, entity tag [ETag]) to a child workflow execution. For each object, the child workflow calls Amazon Textract with the provided bucket and key to extract raw text and relevant fields (name, email address, mailing address, interest area) from the PDF. The child workflow sends the extracted data to Amazon Data Firehose, which is configured to forward data to S3 Tables. Firehose batches the incoming data from the child workflow and writes it to S3 Tables at a preconfigured time interval of your choosing. With data now structured and accessible in S3 Tables, users can easily analyze them using standard SQL queries with Amazon Athena or business intelligence like Amazon QuickSight.\nThe data-processing workflow EventBridge Scheduler starts new Step Functions workflows at regular intervals. The timeline for this schedule is flexible. However, When setting up your schedule, make sure the frequency aligns with how far back your state machine is configured to look for PDFs. For example, if your state machine checks for PDFs from the past week, you’d want to schedule it to run weekly. The Step Functions workflow subsequently performs the following three steps (note that these steps are steps 4, 5, 6, and 7 in the preceding workflow diagram:\nExtract relevant user data from the PDFs. Send the extracted user data to Firehose. Write the data to S3 Tables in Apache Iceberg table format. The following diagram illustrates this workflow.\nLet’s look at each step of the preceding workflow in more detail.\nExtract relevant user data from PDF documents Step Functions uses Distributed Map to process PDFs concurrently in parallel child workflows. It accepts input from JSON, JSONL, CSV, Parquet files, Amazon S3 manifest files stored in Amazon S3 (used to specify particular files for processing), or an Amazon S3 bucket prefix (allows iteration over file metadata for all objects under that prefix). The Step Functions automatically handles parallelization by splitting the dataset and running child workflows for each item, with the ItemBatcher field allowing to group multiple PDFs into a single child workflow execution (e.g., 10 PDFs per batch) to optimize performance and cost.\nThe following screenshot of the Step Functions console shows the configuration for Distributed Map. For example, we have configured Distributed Map to process 10 customer interest PDFs in a single child workflow.\nThe following image shows one example of these scanned PDFs, which includes the customer information that this post’s solution processes.\nEach child workflow then calls the Amazon Textract AnalyzeDocument API with specific queries to extract customer information.\n{ \u0026#34;Document\u0026#34;: { \u0026#34;S3Object\u0026#34;: { \u0026#34;Bucket\u0026#34;: \u0026#34;\u0026lt;input PDFs bucket\u0026gt;\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;{% $states.input.Key %}\u0026#34; } }, \u0026#34;FeatureTypes\u0026#34;: [ \u0026#34;QUERIES\u0026#34; ], \u0026#34;QueriesConfig\u0026#34;: { \u0026#34;Queries\u0026#34;: [ { \u0026#34;Alias\u0026#34;: \u0026#34;full_name\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer\u0026#39;s name?\u0026#34; }, { \u0026#34;Alias\u0026#34;: \u0026#34;phone_number\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer’s phone number?\u0026#34; }, { \u0026#34;Alias\u0026#34;: \u0026#34;mailing_address\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer’s mailing address?\u0026#34; }, { \u0026#34;Alias\u0026#34;: \u0026#34;interest\u0026#34;, \u0026#34;Text\u0026#34;: \u0026#34;What is the customer’s interest?\u0026#34; } ] } } The API analyzes each scanned PDF and returns a JSON structure containing the extracted customer information.\nSend the extracted user data to Firehose The child workflow then uses a Firehose PutRecordBatch API action with service integrations to queue the extracted customer information for further processing. The PutRecordBatch action request includes the Firehose stream name and the data records. The data records include a data blob from step 1 that contains extracted customer information, as shown in the following example.\n{ \u0026#34;DeliveryStreamName\u0026#34;: \u0026#34;put_raw_form_data_100\u0026#34;, \u0026#34;Records\u0026#34;: [ { \u0026#34;Data\u0026#34;: \u0026#34;{\\\u0026#34;full_name\\\u0026#34;:\\\u0026#34;Anthony Ayala\\\u0026#34;,\\\u0026#34;phone_number\\\u0026#34;:\\\u0026#34;001-384-925-0701\\\u0026#34;,\\\u0026#34;mailing_address\\\u0026#34;:\\\u0026#34;38548 Joshua Wall Suite 974, East Heatherfort, OH 32669\\\u0026#34;,\\\u0026#34;interest\\\u0026#34;:\\\u0026#34;Fitness Trackers\\\u0026#34;,\\\u0026#34;processed_date\\\u0026#34;:\\\u0026#34;2025-05-01\\\u0026#34;}\u0026#34; }, { \u0026#34;Data\u0026#34;: \u0026#34;{\\\u0026#34;full_name\\\u0026#34;:\\\u0026#34;Becky Williams\\\u0026#34;,\\\u0026#34;phone_number\\\u0026#34;:\\\u0026#34;+1-283-499-2466\\\u0026#34;,\\\u0026#34;mailing_address\\\u0026#34;:\\\u0026#34;227 King Forge Suite 241, East Nathanland, PR 05687\\\u0026#34;,\\\u0026#34;interest\\\u0026#34;:\\\u0026#34;Al Assistants\\\u0026#34;,\\\u0026#34;processed_date\\\u0026#34;:\\\u0026#34;2025-05-01\\\u0026#34;}\u0026#34; } ] } Write the data to S3 Tables in Apache Iceberg table format Firehose efficiently manages data buffering, format conversion, and reliable delivery to various destinations, including Apache Iceberg, raw files in Amazon S3, Amazon OpenSearch Service, or any of the other supported destinations. Apache Iceberg tables can be either self-managed in Amazon S3 or hosted in S3 Tables. Though self-managed Iceberg tables require manual optimization—such as compaction and snapshot expiration—S3 Tables automatically optimize storage for large-scale analytics workloads, improving query performance and reducing storage costs.\nFirehose simplifies the process of streaming data by configuring a delivery stream, selecting a data source, and setting an Iceberg table as the destination. After you’ve set it up, the Firehose stream is ready to deliver data. The delivered data can be queried from S3 Tables by using Athena, as shown in the following screenshot of the Athena console.\nThe query results include all processed customer data from the PDFs, as shown in the following screenshot.\nThis integration demonstrates a powerful, code-free solution for transforming raw PDF forms into enriched, queryable data in an Iceberg table. You can use these data for further analysis.\nConclusion In this post, we showed how to build a scalable, serverless solution for processing PDF documents and exporting the extracted data to S3 Tables by using Step Functions Distributed Map. This architecture offers several key benefits such as reliability, cost-effectiveness, visibility, and maintainability. By leveraging AWS services such as Step Functions, Amazon Textract, Firehose, and S3 Tables, companies can automate their document processing workflows while ensuring optimal performance and operational excellence. This solution can be adapted for various use cases beyond customer interest forms, such as invoice processing, application forms, or any scenario requiring structured data extraction from documents at scale.\nThough this example focuses on processing PDF data and writing to S3 Tables, Distributed Map can handle various input sources including JSON, JSONL, CSV, and Parquet files in Amazon S3; items in Amazon DynamoDB tables; Athena query results; and all paginated AWS List APIs. Similarly, through Step Functions service integrations, you can write results to multiple destinations such as DynamoDB tables by using the PutItem service integration.\nTo get started with this solution, see the associated GitHub repository for deployment instructions and sample code.\n"},{"uri":"https://vdhxi.github.io/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"DISA STIG for Amazon Linux 2023 is now available Today, we announce the availability of a Security Technical Implementation Guide (STIG) for Amazon Linux 2023 (AL2023), developed through collaboration between Amazon Web Services (AWS) and the Defense Information Systems Agency (DISA). The STIG guidelines are important for U.S Department of Defense (DOD) and Federal customers needing strict security compliance derived from the National Institute of Standards and Technology (NIST) 800-53 and related documents. This new technical implementation guide provides detailed Operating System (OS) security hardening configurations for organizations deploying AL2023 in DOD environments and other agencies requiring DISA STIG alignment. The AL2023 STIG provides customers with access to an OS guide that complies with stringent government security standards. This guide for implementing STIG configurations will streamline security processes for organizations seeking robust cybersecurity controls, whether they are needed to maintain DOD compliance or voluntarily adopting these best security practices to enhance their security posture. Implementing the AL2023 DISA STIG with AWS AWS Systems Manager (SSM) and EC2 Image builder offer native solutions for implementing the AL2023 DISA STIG configurations in your environment. For customers with existing AL2023 EC2 workload, they can utilize AWS Systems Manger (SSM) to streamline the STIG implementation. For customers who would like to build STIG compliant AL2023 EC2 instances to use for deployment, they can utilize EC2 Image Builder and automate the application of the AL2023 DISA STIG.\nBuilding STIG-Compliant Images via EC2 Image Builder Customers can utilize EC2 Image builder to enhance and streamline their implementation of the AL2023 DISA STIG. This integrated approach significantly reduces the operational overhead traditionally associated with maintaining STIG compliance. Therefore, our customers can focus on their core missions while maintaining the highest security standards. Our customers can use AWS EC2 Image Builder’s existing Linux hardening components, which now support AL2023 Category I, II, and III findings to automatically create STIG-compliant AL2023 EC2 images with minimal manual intervention. This automation significantly reduces the time and effort typically needed for security hardening implementations. The EC2 Image Builder Linux hardening component extends its proven capabilities to AL2023, providing the same streamlined security configuration process available for other Linux distributions. For more information, refer to the Image Builder documentation.\nAutomating the STIG for Existing Fleets via Systems Manager For existing AL2023 EC2 instances, you can use AWS-managed SSM command documents to automate the implementation of the STIG configurations. . These command documents can be executed through the SSM console, API, or AWS Command Line Interface (AWS CLI). The key mechanism here is the AWS managed Systems Manager command document, which contains the pre-defined STIG configurations. By leveraging these command documents through Systems Manager execution capabilities, customers can systematically deploy and maintain AL2023 STIG configurations across their fleet of EC2 instances. This generates consistent security baselines that meet government and enterprise requirements. This solution is particularly effective for environments with existing AL2023 EC2 instances as it allows customers to implement STIG controls without rebuilding or redeploying instances. For more information about the command document, refer to Apply STIG settings with Systems Manager in the EC2 User Guide.\nThe AL2023 STIG represents the continued commitment of Amazon Linux to providing customers with the security tools and guidance they need to succeed in highly regulated environments. Amazon Linux, in collaboration with DISA is providing their customers with access to authoritative, government-validated security configurations that meet the most demanding compliance requirements.\nReady to implement AL2023 STIG in your environment? Explore our comprehensive documentation and begin streamlining your security compliance journey today. To learn more about STIG hardening for your EC2 instances, refer to STIG compliance for your EC2 instance and for STIG settings that are applied to EC2 Linux instances, refer to the STIG settings for EC2 Linux instances. To apply STIG settings to your AL 2023 EC2 instance, download the AL2023 DISA STIG.\n"},{"uri":"https://vdhxi.github.io/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"Architecting for AI excellence: AWS launches three Well-Architected Lenses at re:Invent 2025 At re:Invent 2025, we introduce one new lens and two significant updates to the AWS Well-Architected Lenses specifically focused on AI workloads: the Responsible AI Lens, the Machine Learning (ML) Lens, and the Generative AI Lens. Together, these lenses provide comprehensive guidance for organizations at different stages of their AI journey, whether you’re just starting to experiment with machine learning or already deploying complex AI applications at scale.\nThe AWS Well-Architected Framework provides the best architectural practices for designing and operating reliable, secure, performance efficient, cost-optimized, and sustainable workloads in the cloud.\nThe Responsible AI Lens: Embedding trust in AI systems The Responsible AI Lens offers a structured approach for developers to assess and track their AI workloads against established best practices, identify potential gaps in their AI implementation and receive actionable guidance to improve their AI systems’ quality and alignment with responsible AI principles. By using the Responsible AI Lens you can make informed decisions that balance business and technical requirements, accelerating your path from AI experimentation to production-ready solutions.\nKey takeaways from the Responsible AI Lens:\nEvery AI system has a Responsible AI consideration: Whether intentionally designed or not, AI systems inherently carry Responsible AI implications that need to be actively managed rather than left to chance. AI systems can be used beyond original intent and may have unintended impacts: Applications often get utilized in ways developers didn’t anticipate, and due to their probabilistic nature, AI systems can produce unexpected outcomes even within intended use cases, making robust Responsible AI decisions essential from the start. Responsible AI is an enabler to innovation and trust: Rather than being a constraint, Responsible AI practices can accelerate innovation by proactively building stakeholder and customer trust and reducing downstream risks. The Responsible AI Lens serves as the foundational guidance for AI development activities, providing critical guidelines that inform both the Machine Learning Lens and the Generative AI Lens implementations.\nThe Machine Learning Lens: Foundation for ML workloads The Machine Learning Lens provides you with a set of established cloud-agnostic best practices in the form of Well-Architected Framework pillars for each machine learning (ML) lifecycle phase. The updated Machine Learning Lens provides a consistent approach for designing, building, and operating machine learning workloads on AWS. It addresses the full spectrum of ML workloads, from traditional supervised and unsupervised learning to modern AI applications.\nThe updated Machine Learning Lens incorporates the latest AWS ML capabilities (evolved since their introduction in 2023). What’s new in the updated ML Lens:\nEnhanced data and AI collaborative workflows through Amazon SageMaker Unified Studio. AI-assisted development for code generation and productivity enhancement. Distributed training infrastructure for foundation model development and fine-tuning with Amazon SageMaker HyperPod. Model customization capabilities such as knowledge distillation and fine-tuning domain-specific applications using Amazon Bedrock with Kiro and Amazon Q Developer. No-code ML development using Amazon SageMaker Canvas with Amazon Q integration. Improved bias detection with enhanced fairness metrics and Responsible AI capabilities in Amazon SageMaker Clarify. Automated dashboard creation for business insights through Amazon Quick Sight. Modular inference architecture for flexible model deployment with Inference Components. Advanced observability with improved debugging and monitoring capabilities across the ML lifecycle. Enhanced cost optimization for resource management through Amazon SageMaker Training Plans, Savings Plans, and Spot Instance support. You can use the ML Lens wherever you are on your cloud journey. You can choose to apply this guidance either during the design of your ML workloads or after your workloads have entered production as part of the continuous improvement process. These improvements are powered by key AWS services including Amazon SageMaker Unified Studio, Amazon Q, Amazon SageMaker HyperPod, and Amazon Bedrock.\nThe Generative AI Lens: Specialized guidance for foundation models The Generative AI Lens provides a consistent approach for customers to evaluate architectures that use large language models (LLMs) to achieve their business goals. This lens addresses common considerations relevant to model selection, prompt engineering, model customization, workload integration, and continuous improvement. We identify best practices that help you architect your cloud-based applications and workloads according to AWS Well-Architected design principles gathered from supporting thousands of customer implementations. While the Machine Learning (ML) Lens covers the broad spectrum of ML workloads, the Generative AI Lens focuses specifically on foundation models and generative AI applications. The Generative AI Lens provides the best architectural practices for designing and operating generative AI workloads on AWS.\nThe updated Generative AI Lens includes several new additions:\nAmazon SageMaker HyperPod guidance for orchestrating complex, long-running generative AI workflows that includes additional service capabilities. Enhanced Responsible AI preamble with detailed discussion on the eight core dimensions of Responsible AI as described by AWS. Updated data architecture preamble with strategic considerations needed to architect a data system for generative AI workloads. New agentic AI preamble introducing architecture paradigms for agentic systems. Eight architecture scenarios covering common generative AI-powered business applications such as autonomous call centers, knowledge worker co-pilots, and multi-tenant generative AI service systems. The Generative AI Lens builds upon the foundation established by the ML Lens, providing specialized guidance for the unique challenges and opportunities presented by foundation models and generative AI applications.\nImplementation strategy for Well-Architected AI/ML guidance: A unified approach The new lenses – Responsible AI Lens, Machine Learning Lens, and Generative AI Lens – work together to provide comprehensive guidance for AI development. The Responsible AI Lens guides safe, fair, and secure AI development. It helps balance business needs with technical requirements, streamlining the transition from experimentation to production. The Machine Learning Lens guides organizations in evaluating workloads across both modern AI and traditional machine learning approaches. Recent updates focus on key areas including enhanced data and AI collaborative workflows, AI-assisted development capabilities, large-scale infrastructure provisioning, and customizable model deployment. The Generative AI Lens helps customers evaluate large language model (LLM) based architectures and its updates include guidance for Amazon SageMaker HyperPod users, new insights on agentic AI, and updated architectural scenarios.\nWhat are the next steps? The launch of these new lenses at re:Invent 2025 helps organizations build AI systems that are responsible, trustworthy, powerful, and effective. By providing comprehensive guidance across the full spectrum of AI workloads, AWS supports organizations to accelerate their AI initiatives while maintaining the highest standards of responsible AI and technical excellence.\nLearn more about the AWS Well-Architected Framework and implement the best practice guidance provided using the GitHub repository. These lenses are practical tools designed to help you build AI systems that deliver real business value while maintaining the highest standards of ethics, security, and operational excellence.\nFor additional reading, refer to the AWS Well-Architected Framework and pillar whitepapers, or contact your AWS Solutions Architect or Account Representative for support on implementing these lenses in your organization.\n"},{"uri":"https://vdhxi.github.io/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"Announcing the updated AWS Well-Architected Generative AI Lens We are delighted to announce an update to the AWS Well-Architected Generative AI Lens. This update features several new sections of the Well-Architected Generative AI Lens, including new best practices, advanced scenario guidance, and improved preambles on responsible AI, data architecture, and agentic workflows.\nThe AWS Well-Architected Framework provides architectural best practices for designing and operating generative AI workloads on AWS. The Generative AI Lens uses the Well-Architected Framework to outline the steps for performing a Well-Architected Framework review for your generative AI workloads.\nThe Generative AI Lens provides a consistent approach for customers to evaluate architectures that use large language models (LLMs) to achieve their business goals. This lens addresses common considerations relevant to model selection, prompt engineering, model customization, workload integration, and continuous improvement. Specifically excluded from the Generative AI Lens are best practices associated with model training and advanced model customization techniques. We identify best practices that help you architect your cloud-based applications and workloads according to AWS Well-Architected design principles gathered from supporting thousands of customer implementations.\nThe Generative AI Lens joins a collection of Well-Architected lenses published under AWS Well-Architected Lenses. For more information on the lens itself, check out the launch announcement post.\nWhat has changed in the updated Generative AI Lens? The updated Generative AI Lens incorporates several new additions for customers to review. These additions keep the lens on-pace with the rapidly growing area of generative AI, helping customers stay up to date with architectural best practices.\nAmazon SageMaker HyperPod guidance The updated lens features additional guidance for users of Amazon SageMaker HyperPod. SageMaker HyperPod is a highly resilient model training and hosting service that you can use to orchestrate complex, long-running generative AI workflows in the cloud. These workflows could be foundation model pre-training or serving model inference at scale.\nWe are excited to announce additional guidance for customers using SageMaker HyperPod in the Generative AI Lens. This guidance is built into the existing best practices, expanding the guidance for covered services to include SageMaker capabilities. This guidance joins the existing guidance on Amazon Bedrock, Amazon Q Business, Amazon Q Developer, and Amazon SageMaker AI.\nResponsible AI preamble The updated responsible AI preamble now includes a detailed discussion on the eight core dimensions of responsible AI as described by AWS. Customers can now learn more about the eight dimensions of responsibly developed AI systems directly within the lens. This is required reading for customers in all stages of their generative AI journey.\nData architecture preamble The updated data architecture preamble reviews strategic considerations associated with a modern data architecture supporting generative AI workloads. This section provides customers with a view into the high-level decisions and considerations needed to architect a data system that services generative AI workloads.\nAgentic AI preamble New to the generative AI lens is the agentic AI preamble. Agentic systems, while technically classified as a subset of distributed computing, play an important role in modern generative AI workloads. This preamble introduces customers to a sampling of architecture paradigms common in agentic systems powered by foundation models.\nScenarios The Generative AI Lens now includes eight architecture scenarios. These scenarios cover a range of common generative AI powered business applications, including autonomous call centers, knowledge worker co-pilots, and multi-tenant generative AI service systems. The scenario section provides specific guidance for applying generative AI technologies to common business problems. The following image is an example of one of the new scenarios now included in the Generative AI Lens.\nWho should use the Generative AI Lens? The Generative AI Lens is useful to many roles. Business leaders can use this lens to acquire a broader appreciation of the end-to-end implementation and benefits of generative AI. Data scientists and engineers can read this lens to understand how to use, secure, and gain insights from their data at scale. Risk and compliance leaders can understand how generative AI is implemented responsibly by providing compliance with regulatory and governance requirements.\nNext steps The updated Well-Architected Generative AI Lens is available now. Use the lens as a framework to verify that your generative AI workloads are architected with operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability in mind.\nIf you require support on the implementation or assessment of your generative AI workloads, please contact your AWS Solutions Architect or Account Representative.\nSpecial thanks to everyone across the AWS Solution Architecture, AWS Professional Services, and Machine Learning communities who contributed to the updated Generative AI Lens. These contributions encompassed diverse perspectives, expertise, backgrounds, and experiences in developing the new AWS Well-Architected Generative AI Lens.\nFor additional reading, refer to the AWS Well-Architected Framework and pillar whitepapers, or use the AWS Well-Architected Machine Learning Lens and its custom lens accessible from the AWS Well-Architected Tool.\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.4-distribution/5.4.1-ssl-certificate/","title":"Certificate Manager","tags":[],"description":"","content":"AWS Certificate Manager (ACM) To use HTTPS, we need an SSL certificate.\nSteps Access ACM Console.\nSelect Request a certificate.\nSelect Request a public certificate. Enter Domain name (e.g., *.example.com). Select DNS validation.\nClick Request.\nAfter requesting, click on the certificate ID, select Create records in Route 53 for automatic validation. Wait for status to change to Issued.\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.1-preparation/5.1.1-vpc/","title":"Create VPC","tags":[],"description":"","content":"Initialize Virtual Private Cloud (VPC) The system will run inside a Virtual Private Cloud (VPC) to ensure security and resource isolation. We will create a VPC with Public Subnets (for Load Balancers) and Private Subnets (for EC2, RDS).\nSteps Access the AWS Console and search for the VPC service. Select Create VPC. Choose VPC and more configuration to quickly create a VPC along with subnets and Route Tables. Fill in the information: Name tag: Auction-VPC IPv4 CIDR block: 10.0.0.0/16 Number of Availability Zones (AZs): 2 (to ensure high availability) Number of public subnets: 2 Number of private subnets: 2 NAT gateways: None (or 1 per AZ if EC2 in private subnet needs internet access to download packages, but to save costs in this lab, you can choose None or 1). Click Create VPC. Wait for the initialization process to complete. "},{"uri":"https://vdhxi.github.io/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://vdhxi.github.io/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.5-deploy/5.5.2-gitlab/","title":"GitLab CI","tags":[],"description":"","content":"Configure GitLab CI Steps 1. Prepare Variables Go to Settings -\u0026gt; CI/CD -\u0026gt; Variables on the GitLab Repository. Add necessary variables:\nEC2_IP SSH_PRIVATE_KEY 2. Configuration file .gitlab-ci.yml Create a .gitlab-ci.yml file at the root of the project. The workflow includes:\nBuild: Build JAR file (Spring Boot) or Docker Image. Deploy: Copy file to EC2 and restart service. Example of a successful pipeline: "},{"uri":"https://vdhxi.github.io/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Vo Duc Hoang\nPhone Number: 0366934122\nEmail: hoangvdse182010@gmail.com\nUniversity: FPT University Ho Chi Minh\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://vdhxi.github.io/5-workshop/5.1-preparation/","title":"Preparation","tags":[],"description":"","content":"Environment Preparation Before installing the main services, we need to prepare the Networking layer and Access Rights (IAM) for the resources.\nContent VPC: Create a Virtual Private Cloud to isolate the network for the system. IAM: Create necessary Roles for EC2 to access S3, Rekognition, and Textract. "},{"uri":"https://vdhxi.github.io/5-workshop/5.5-deploy/5.5.1-setup-ec2/","title":"Setup EC2 Instance Environment","tags":[],"description":"","content":"Environment Setup for EC2 Server In this section, we will assign an IAM Role to the EC2 instance and install necessary software such as Java and MariaDB to prepare for application deployment.\n1. Assign IAM role to EC2 To allow the EC2 instance to access other AWS services (e.g., Session Manager for connection without opening SSH ports, or accessing S3, RDS), we need to assign an appropriate IAM Role.\nAccess EC2 Dashboard, select the Instance you just created. Select Actions -\u0026gt; Security -\u0026gt; Modify IAM role. Select the IAM Role created in previous steps (e.g., EC2RoleForSSM) and click Update IAM role. 2. Environment Setup Connect to the EC2 Instance (using Session Manager or SSH). Then perform the following steps sequentially:\n2.1. Update System Run the following command to update to the latest software packages:\nsudo dnf update -y 2.2. Install Java Our application runs on the Java platform, so installing the Java Development Kit (JDK) is required. Here we use Amazon Corretto 21 (headless version is more suitable for command-line interfaces without graphics).\nsudo dnf install java-21-amazon-corretto-headless -y Check Java version after installation:\njava -version 2.3. Install MariaDB Client and Initialize Database Install MariaDB client to connect and interact with RDS.\nsudo dnf install mariadb105 -y Connect to the created RDS database instance. Replace \u0026lt;rds-endpoint\u0026gt;, \u0026lt;username\u0026gt; with your actual information:\nmysql -h \u0026lt;rds-endpoint\u0026gt; -u \u0026lt;username\u0026gt; -p After successfully entering the password, create the database for the application:\nCREATE DATABASE tickets; SHOW DATABASES; 2.4. Setup Service for Auto-starting Java Springboot Application Run the following command to create a service file\nsudo nano /etc/systemd/system/\u0026lt;service-name\u0026gt;.service Enter the service file content, configure environment variables for the application\nUse key combination Ctrl + O, Enter and Ctrl + X to save and exit.\nUse the following commands to apply changes\nsudo systemctl daemon-reload sudo systemctl restart \u0026lt;service-name\u0026gt; Check status using command status Use command enable so the service automatically runs every time the EC2 instance starts\nsudo systemctl enable \u0026lt;service-name\u0026gt; Command to set timezone to synchronize (in case Java application needs to match Vietnam time)\nsudo timedatectl set-timezone Asia/Ho_Chi_Minh Check result with command\ndate "},{"uri":"https://vdhxi.github.io/1-worklog/","title":"Worklog","tags":[],"description":"","content":"In this page you will need to introduce your worklog how? You complete the program in how many weeks? What did you do during those weeks?\nTypically and also as a standard, a worklog is carried out in about 3 months (during the internship period) with the content of the weeks as follows:\nWeek 1: Get familiar with AWS and basic services in AWS\nWeek 2: Practice using AWS services\nWeek 3: Design auction web database, select AWS services\nWeek 4: Build auction web backend, registration, login, security, account recovery features. Group meeting to choose group project topic\nWeek 5: Build auction web backend, system category management features. Design group project database\nWeek 6: Build auction web backend, auction feature. Complete backend for group project\nWeek 7: Build auction web system, transaction wallet management feature.\nWeek 8: Build auction web system, delivery feature.\nWeek 9: Complete backend for auction web, build project interface\nWeek 10: Complete home page interface, user interface for auction web.\nWeek 11: Complete admin, staff interface for auction web.\nWeek 12: Complete project, deploy to cloud environment, testing.\n"},{"uri":"https://vdhxi.github.io/1-worklog/1.1-week1/","title":"Worklog Week 1","tags":[],"description":"","content":"Week 1 Objectives: Connect and get to know members of the First Cloud Journey. Understand basic AWS services, how to use console \u0026amp; CLI. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Get to know FCJ members - Read and note the rules and regulations at the internship unit 08/09/2025 08/09/2025 Tue - Learn what AWS is and the types of services AWS provides, things to note when starting to use AWS services 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/https://www.youtube.com/@AWSStudyGroup Wed - Create AWS Free Tier account - Learn how to register AWS Account, configure account security 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/https://www.youtube.com/@AWSStudyGroup Thu - Learn how to manage costs when using AWS services - Practice with AWS Budget 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ Fri - Learn about Region, Availability Zones concepts - Learn basic knowledge about VPC, Public subnet, Private subnet 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/https://www.youtube.com/@AWSStudyGroup Week 1 Achievements: Understood what AWS is and grasped basic service groups:\nCompute Storage Networking Database AI Successfully created and configured AWS Free Tier account.\nGot familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nCreated Budget plan, know how to track monthly costs for used services.\nCapable of connecting between web interface and CLI to manage AWS resources in parallel.\nUnderstood concepts of Region, Availability Zone\nUnderstood how resources in VPC public subnet, private subnet communicate with each other and with the external internet environment\n"},{"uri":"https://vdhxi.github.io/1-worklog/1.2-week2/","title":"Worklog Week 2","tags":[],"description":"","content":"Week 2 Objectives: Understand necessary knowledge when hosting a website on a cloud environment Successfully build and deploy a website on a cloud environment Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about EC2 virtual servers - Practice creating EC2 servers, configuring security groups, connecting to EC2 using mobaXterm- Hosting backend on EC2 server 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ Tue - Learn about AWS S3 - Practice creating S3 buckets, uploading, managing files - Practice web hosting with S3 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ Wed - Learn about AWS RDS - Practice connecting EC2 and RDS 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn about Route53, CloudFront, AWS SES - Configure personal DNS management via Route53- Configure AWS SES to send mail 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice hosting website combining EC2, AWS S3, Route53, CloudFront and RDS- Config security group to be able to SSH access to the server for management - Config elastic IP for EC2 server 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Learned knowledge about basic services when building a website Applied learned services to website hosting Understood how to successfully deploy a website on a cloud environment Configured security group to access SSH to the website, configure and manage the server Installed Java for hosting Java backend Installed certbot to register SSL Installed nginx to redirect access to the server from port 8080 HTTP protocol to port 443 HTTPS protocol "},{"uri":"https://vdhxi.github.io/1-worklog/1.3-week3/","title":"Worklog Week 3","tags":[],"description":"","content":"Week 3 Objectives: Learn and practice automated CI/CD process Build an auction website project based on services that AWS currently provides Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about Gitlab CI, how Gitlab CI operates- Practice using Gitlab CI to automatically deploy code when committing 22/09/2025 22/09/2025 https://docs.gitlab.com/ci/ https://codefresh.io/learn/gitlab-ci/ Tue - Analyze auction website project: - Use cases in the system - User roles of the system - Services to be used to implement features 23/09/2025 23/09/2025 Wed - Learn about AWS Rekognition and AWS Textract - Practice integrating Rekognition and Textract into Springboot application 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ https://github.com/awsdocs/aws-doc-sdk-examples/tree/main/javav2/example_code/rekognition Thu - Design database for auction web system 25/09/2025 26/09/2025 Fri - Design database for auction web system (continued) 26/09/2025 26/09/2025 Week 3 Achievements: Applied CI/CD process in project development Grasped how to integrate and use Rekognition and Textract in Springboot application Analyzed requirements, use cases, user roles in the system Designed database suitable for the scale of the system "},{"uri":"https://vdhxi.github.io/1-worklog/1.4-week4/","title":"Worklog Week 4","tags":[],"description":"","content":"Week 4 Goals: Build APIs for login, registration, and account recovery Tasks to be implemented this week: Day Task Start Date End Date Resource Mon - Research JWT Token - Design login process combined with real user verification, avoiding creation of multiple virtual accounts - Build account verification API based on Spring Security, using Jwt token 29/09/2025 29/09/2025 https://www.youtube.com/watch?v=1XC5WPQkXek\u0026list=PL2xsxmVse9IaxzE8Mght4CFltGOqcG6FC\u0026index=9 Tue - Research Java Mail Sender - Implement email sending feature using Java Mail Sender - Build API to send email to specific address - Implement email OTP security feature 30/09/2025 30/09/2025 https://mailtrap.io/blog/java-send-email/ Wed - Research Google Authenticator TOTP 2-step verification system - Integrate TOTP 2-step verification feature into the system - Build OTP verification API 1/10/2025 1/10/2025 https://www.youtube.com/watch?v=2m2yuaomCTc Thu - Design safe and secure password recovery process - Implement code, combine with other APIs to implement password recovery process 2/10/2025 2/10/2025 Fri - API testing, fixing bugs during implementation to complete features and prepare for use in other processes requiring security 3/10/2025 3/10/2025 Week 4 Results: Built APIs serving processes: Registration: checked for duplicates, while limiting spam creation of junk accounts. Login: built login process, user information verification based on JWT token. Also integrated TOTP to increase security. Password recovery: built secure password recovery process, only the real owner can recover password by verifying via email and TOTP. Designed information verification steps, linked together to create a strict security process for security purposes, only the owner can perform. "},{"uri":"https://vdhxi.github.io/1-worklog/1.5-week5/","title":"Worklog Week 5","tags":[],"description":"","content":"Week 5 Goals: Design and implement APIs to: Change account security information Manage (add, edit, delete) system categories Manage (edit) user information Build account verification process using Rekognition and Textract Manage (add, edit, delete) auctions Understand and apply Redis to cache queries to increase system performance Tasks to be implemented this week: Day Task Start Date End Date Resource Mon - Build API to change login information, account security information 6/10/2025 6/10/2025 Tue - Build API to manage system categories, manage user information - Build user information verification process using Rekognition and Textract 7/10/2025 7/10/2025 Wed - Build API to manage auctions 8/10/2025 8/10/2025 Thu - Research Redis - Use Redis to cache queries to increase performance, reduce load on database 9/10/2025 9/10/2025 https://spring.io/projects/spring-data-redis#learn https://viblo.asia/p/huong-dan-spring-boot-redis-aWj53NPGl6m https://kungfutech.edu.vn/bai-viet/spring-boot/su-dung-redis-trong-spring-boot Fri - API testing, fixing bugs arising during development 10/10/2025 10/10/2025 Week 5 Results: Completed implementation of the following APIs: API for users to change password, change email, verify password. API to manage addresses, product categories: Add new, edit, disable. API for users to update personal information: username, avatar. API for staff to edit user information in data fields requiring access in special use cases: authentication status, lock (unlock) login, remove transaction limit time. API to verify user information based on selfie and ID documents: use Rekognition\u0026rsquo;s Face Compare feature to match faces and Textract\u0026rsquo;s Analyze Document ID to extract necessary data. Configured Redis to cache results when querying database, reducing load on database system as the auction web is a high-performance system. "},{"uri":"https://vdhxi.github.io/1-worklog/1.6-week6/","title":"Worklog Week 6","tags":[],"description":"","content":"Week 6 Goals: Continue building backend API for auction web system Tasks to be implemented this week: Day Task Start Date End Date Resource Mon - Fix bugs in auction web APIs 13/10/2025 13/10/2025 Tue - Fix bugs in auction web APIs 14/10/2025 14/10/2025 Wed - API testing, checking and fixing remaining bugs 15/10/2025 15/10/2025 Thu - Build bid placement API, using redis lock to fix race condition - Redesign database, modify relationships between entities - Create basic APIs to manage user wallets in the system 16/10/2025 16/10/2025 https://www.anhdh.net/blog/redis-distributed-locking Fri - Build feature to send support requests, handle user support requests - Build API for staff to stop invalid auctions 17/10/2025 17/10/2025 Week 6 Results: Continued to perfect APIs for auction web system: Built bid placement API applying Redis distributed lock mechanism Redesigned database to fit current system entities Built APIs serving staff role use cases in the system: Stop invalid auctions Handle user tickets "},{"uri":"https://vdhxi.github.io/1-worklog/1.7-week7/","title":"Worklog Week 7","tags":[],"description":"","content":"Week 7 Goals: Implement e-wallet integrated into the auction system Tasks to be implemented this week: Day Task Start Date End Date Resource Mon - Integrate VnPay for system deposit function 20/10/2025 20/10/2025 https://sandbox.vnpayment.vn/apis/docs/thanh-toan-pay/pay.html Tue - Fix VnPay bugs 21/10/2025 21/10/2025 Wed - Build auxiliary APIs for main functions: + Verify PIN code + Change PIN code + Change daily transfer limit - Create API to view transaction history 22/10/2025 22/10/2025 Thu - Combine security information verification APIs to implement features: + Transfer money + Withdraw money 23/10/2025 23/10/2025 Fri - API testing, checking and fixing bugs appearing during development 24/10/2025 24/10/2025 Week 7 Results: Completed implementation of e-wallet system into the auction system, serving many other logics with functions such as: Deposit Transfer money Withdraw money View transaction history "},{"uri":"https://vdhxi.github.io/1-worklog/1.8-week8/","title":"Worklog Week 8","tags":[],"description":"","content":"Week 8 Objectives: Build delivery process after auction session completion Build constraint mechanism, check conditions to avoid users spamming virtual auction sessions in the system Tasks to implement this week: Day Task Start Date End Date Resources Mon - Add check conditions in data validation mechanism for users to create new auction sessions Edit logic code in related functions 27/10/2025 27/10/2025 Tue - Create APIs to update post-auction delivery process: + Seller creates order + User confirms order + Allow seller/buyer to update order 28/10/2025 28/10/2025 Wed - Create APIs allowing order status updates: + Completed + Cancel order by buyer/seller + Create return/refund request 29/10/2025 29/10/2025 Thu - Implement return/refund process handled by staff role - Update wallet balance and transaction history of users in cases 30/10/2025 30/10/2025 Fri - API testing, check and fix errors appearing during development 31/10/2025 31/10/2025 Week 8 Achievements: Completed building post-auction delivery process:\nCreate order. Buyer/seller confirms, updates information. Completed implementation of confirmation process for completion, cancellation, return/refund. Updated auction session creation conditions, edited logic code in other APIs to fit business rules.\n"},{"uri":"https://vdhxi.github.io/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives: Configure websocket in backend to send real-time notifications Build interface for auction web system Tasks to implement this week: Day Task Start Date End Date Resources Mon - Implement websocket to send notifications when there are new events in backend 3/11/2025 3/11/2025 https://spring.io/guides/gs/messaging-stomp-websocket Tue - Build main interfaces for auction web system: + Home page + Login page + Registration page + Forgot password page + Auction list page + Auction detail page 4/11/2025 4/11/2025 Wed - Build user interface for auction web system (1) 5/11/2025 6/11/2025 Thu - Build user interface for auction web system (2) 6/11/2025 6/11/2025 Fri - Link API for main pages 7/11/2025 7/11/2025 Week 9 Achievements: Configured WebSocket in backend to send realtime messages to frontend when the following events occur:\nUpdate new auction session. Update price, new status for auction session. Balance fluctuation notification. Order notification after successful auction. Built interface for main pages, successfully linked API for use cases:\nRegistration Login Login with OTP verification code Recover login password View current auction sessions View details of an auction session Built user interface including pages:\nInfo management: Update info, verify personal info. Auction session management: Create new, update, cancel session. Address management: Create new, update, delete. Wallet management: Create new, top up, transfer, withdraw, change PIN, change limit, view transaction history. Order management: View, edit, confirm status. Support request management: Create new, view. Security info management: Change password, email, enable and disable 2-step verification. "},{"uri":"https://vdhxi.github.io/5-workshop/5.2-database-storage/5.2.2-elasticache/","title":"Amazon ElastiCache","tags":[],"description":"","content":"Initialize Amazon ElastiCache (Redis) Redis helps cache frequent queries and store user sessions.\nSteps 1. Create Subnet Group Go to ElastiCache Dashboard -\u0026gt; Subnet groups -\u0026gt; Create subnet group. 2. Create Security Group Create a Security Group allowing port 6379 from EC2. 3. Create Redis Cluster Select Redis OSS caches -\u0026gt; Create cache.\nSelect Configure and create a new cluster.\nSelect Cluster mode disabled (for simplicity and cost savings). Configure Redis info. Configure Node type, e.g., cache.t3.micro. Select Subnet group. Select Security Group. Click Create.\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.4-distribution/5.4.2-alb/","title":"Application Load Balancer","tags":[],"description":"","content":"Application Load Balancer (ALB) ALB will distribute traffic to EC2 instances and handle SSL termination.\nSteps 1. Create Security Group for ALB Allow HTTP (80) and HTTPS (443) from 0.0.0.0/0. 2. Create Target Group Create a Target Group of type Instances. Protocol HTTP/8080 (Backend port). Register EC2 instances into the Target Group. 3. Create Load Balancer Go to Load Balancers -\u0026gt; Create load balancer.\nSelect Application Load Balancer.\nSelect VPC and Public Subnets.\nSelect the created Security Group.\nConfigure Listeners:\nHTTP:80 -\u0026gt; Redirect to HTTPS (recommended). HTTPS:443 -\u0026gt; Forward to Target Group. In the HTTPS listener section, select the created ACM Certificate. Click Create load balancer.\nAfter creating the Load Balancer, you can reconfigure the security group for the EC2 instance to only accept inbound rules from the Load Balancer. "},{"uri":"https://vdhxi.github.io/5-workshop/5.1-preparation/5.1.2-iam/","title":"Create IAM Role","tags":[],"description":"","content":"Create IAM Role for EC2 EC2 needs access to S3 to retrieve code/images, and permissions to call Rekognition and Textract APIs. Instead of storing Access Keys in the code, we will use an IAM Role attached to the EC2 instance.\nSteps Access IAM Dashboard. Select Roles -\u0026gt; Create role. In the Trusted entity type step, select AWS service. In Use case, select EC2. Click Next. In the Add permissions step, search and select the following Policies: AmazonS3FullAccess (Or a policy limited to the project bucket only). AmazonRekognitionFullAccess. AmazonTextractFullAccess. AmazonSSMManagedInstanceCore (To remote into EC2 via Session Manager if needed). Click Next. Name the Role Auction-EC2-Role. Review and click Create role. "},{"uri":"https://vdhxi.github.io/5-workshop/5.2-database-storage/","title":"Database &amp; Storage","tags":[],"description":"","content":"Database and Storage Setup In this section, we will initialize:\nAmazon RDS: Relational Database (MySQL). Amazon ElastiCache: Redis cache. Amazon S3: Object storage (images, source code). These services will serve as the backend data storage for the application.\n"},{"uri":"https://vdhxi.github.io/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Auction system built in AWS Cloud infrastructure Auction web system built on Amazon Web Service cloud platform 1. Executive Summary The Auction system is designed by a FPTU student in Ho Chi Minh City and operates on the AWS Cloud platform. The platform utilizes AWS services to build an online auction marketplace with a user-friendly interface, easy to use and suitable for everyone.\n2. Problem Statement Current Problem Currently, auction systems have not reached many people due to difficulties in accessibility. This project was born to bring a transparent live auction platform that is friendly and accessible to everyone.\nSolution The platform uses AWS CloudFront and S3 Storage combined with ReactJS to provide the web interface, with EC2 servers handling all processing tasks on the Springboot platform, Amazon S3 for storing public and private data, and AWS RDS for database storage. Combined with Amazon Rekognition and Textract to extract information and verify user information to ensure fairness. With this platform, users can register new accounts, verify identities, and participate in exciting auctions on the platform.\nBenefits and Return on Investment (ROI) The project brings an online auction platform that is easily accessible to everyone. Estimated monthly cost is $59.37 USD (according to AWS Pricing Calculator). No additional development costs incurred.\n3. Solution Architecture The platform applies AWS architecture for data management. Public data is stored in public S3 buckets and displayed to users via CloudFront and S3 with ReactJS. All processing operations are performed on EC2 with the Springboot platform. Identity information is processed by Amazon Rekognition and Textract and then stored in private S3 buckets.\nAWS Services Used\nAWS VPC: Create a private virtual network environment. AWS Route 53: Route user traffic. AWS CloudFront: CDN helps accelerate page loading speed, reduce web access latency. AWS Load Balancing: Receive requests from the internet and route to EC2, stabilizing the application. Amazon EC2: Run springboot application to handle backend processing, communicate with database (RDS), cache queries (ElastiCache), call AI services (Rekognition, Textract) and process auctions. Amazon S3: Hosting Frontend: Store frontend source code (ReactJS, Tailwind) for CloudFront distribution. Data storage: 2 buckets (public/private) to store images uploaded by users for auctions and account verification. Amazon ElastiCache: Cache memory, helping store queries to reduce load for database and accelerate API response speed. Amazon RDS: Store main system data, placed in private subnet. Amazon Rekognition: AI image analysis service, performing Face Compare between selfie photos and ID photos to verify identity (eKYC). Amazon Textract: Text extraction service from documents. The system uses Textract to automatically read (OCR) and extract information from ID photos to automatically fill for users. Amazon SES: Backend uses this service to send account verification emails (OTP), auction winning notifications or other system notifications to users. Amazon CloudWatch: Service for monitoring and log management. Component Design\nData Ingestion: Data from users. Data Storage: Data stored in 2 S3 buckets (1 for public and 1 for private - accessed via presigned url) Data Processing: EC2 performs data processing. Web Interface: Amazon S3 stores ReactJS application. 4. Technical Implementation Implementation Stages The project includes the following stages:\nResearch and Architecture Design: Research and design AWS architecture, identify services to be used, design database. Cost Calculation and Feasibility Check: Use AWS Pricing Calculator to estimate and adjust. Development, Testing, Deployment: Program Springboot and ReactJS application, then test in local environment. Deployment on AWS Cloud Environment: Set up Gitlab CI, set up cloud environment and deploy. Technical Requirements\nJava 21 Springboot AWS SDK (S3, Rekognition, Textract, SES) MySQL RDS ReactJS/Vite/TypeScript/Tailwind Gitlab, Gitlab runner CI Postman, CloudWatch 5. Roadmap \u0026amp; Implementation Milestones Pre-internship (Month 0): 1 month for planning and evaluating the old station. Internship (Month 1–3): Month 1: Learn AWS and design architecture, design database, implement API construction. Month 2: Implement API construction, build interface. Month 3: Deploy on cloud environment, test, put into use. 6. Budget Estimate Costs can be viewed on AWS Pricing Calculator\nInfrastructure Costs\nAmazon Route53: $0.5/month (1 hosted zone) S3 Standard: $0.72/month (10 GB, 30000 request GET, 1000 request PUT, 5 GB Transfer out) Application Load Balancer: $18.80/month (data process 50GB) Amazon EC2 (t3.medium): $16.35/month (3yr, no upfront) Amazon ElastiCache (cache.t3.micro): $9.49/month (3yr, no upfront) Amazon RDS: $8.38/month Rekognition: $0.13/month (100 FaceCompare) Textract: $5/month (200 Pages document) Total: $59.37/month.\n7. Risk Assessment Risk Matrix\nNetwork Loss: High impact, low probability. Budget Overrun: Medium impact, low probability. Mitigation Strategy\nCost: AWS budget alerts, service optimization. Contingency Plan\nPeriodic backups in case of incidents. Use CloudFormation to restore cost-related configurations. 8. Expected Results Long-term Value: Can be reused for other projects in the future.\nMore information at Proposal Template\n"},{"uri":"https://vdhxi.github.io/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives: Complete user page Complete staff page Tasks to implement this week: Day Task Start Date End Date Resources Mon - Link API for user page (1) 10/11/2025 10/11/2025 Tue - Link API for user page (2) 11/11/2025 11/11/2025 Wed - Link API for user page (3): - Build interface for staff role 12/11/2025 12/11/2025 Thu - Build interface for staff role 13/11/2025 13/11/2025 Fri - Link API for staff role page 14/11/2025 14/11/2025 Week 10 Achievements: Completed user page with fully implemented functions:\nUpdate information Verify account Manage auction sessions: create new, edit, cancel, view participating sessions, watch auction sessions, view history Manage addresses: create new, update, delete Manage wallet: top up, transfer, withdraw, view transaction history Manage orders: create new, update information, update status, send refund request Manage support requests: create new, view history Manage security information: change password, email, enable and disable 2-step verification Completed staff page with fully implemented functions:\nManage users: view information, update verification status, remove account restrictions Manage support requests: view support requests, monitor orders, process refund requests "},{"uri":"https://vdhxi.github.io/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Create admin page and complete it Implement websocket to receive realtime message notifications from backend Tasks to implement this week: Day Task Start Date End Date Resources Mon - Create admin page - Link APIs 17/11/2025 17/11/2025 Tue - Implement websocket to receive realtime notifications from backend - Update frontend interface to fit information needing display 18/11/2025 18/11/2025 Wed - Fix websocket error when receiving message from backend, update data when receiving message from backend 19/11/2025 19/11/2025 Thu - Fix backend error when sending message to frontend, reconfigure websocket to allow frontend to subscribe to websocket 20/11/2025 20/11/2025 Fri - Configure, fix json error with redis in backend causing display error in frontend - Update auction detail page interface in frontend 21/11/2025 21/11/2025 Week 11 Achievements: Completed admin page to manage system\nManage categories: create new, edit, delete Manage addresses: create new, edit, delete Implemented websocket to receive realtime messages from backend to update information quickly:\nUpdate information when there is a new auction Update price and end time, winner when a new bid is placed for an auction session Update wallet balance information and transaction history Update system notifications Configured websocket and redis:\nAllow frontend to receive private notifications Fix Json parse error when caching data with redis "},{"uri":"https://vdhxi.github.io/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Test auction web system, fix errors during development Deploy system to cloud environment Tasks to implement this week: Day Task Start Date End Date Resources Mon - Adjust interface, buttons, components, font size 24/11/2025 24/11/2025 Tue - Fix logic errors in backend: + Adjust balance deposit mechanism when creating new auction + Fix logic errors related to data verification when placing bid 25/11/2025 25/11/2025 Wed - Deploy system to AWS cloud environment, configure environment variables 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/vi/ Thu - Fix system errors, adjust Url mapping when deploying on AWS cloud environment - Adjust frontend and backend to fix race condition causing duplicate data 27/11/2025 27/11/2025 Fri - Optimize backend code to increase performance, optimize query, remove unnecessary services 28/11/2025 28/11/2025 Week 12 Achievements: Tested entire auction web system, continued to complete remaining functions, interfaces Deployed system to AWS cloud environment, using services: AWS EC2 AWS RDS AWS S3 storage AWS Route 53 AWS CloudFront Optimized system by redesigning logic code "},{"uri":"https://vdhxi.github.io/5-workshop/5.4-distribution/5.4.3-cloudfront/","title":"Amazon CloudFront","tags":[],"description":"","content":"Amazon CloudFront CloudFront helps distribute static content from S3 with low latency.\nSteps Access CloudFront Console -\u0026gt; Create distribution.\nIn Origin domain, select the Frontend S3 Bucket. In Origin access, select Legacy access identities or OAC (Origin Access Control) to restrict direct user access to S3. Select Create new OAC.\nViewer protocol policy: Redirect HTTP to HTTPS.\nAllowed HTTP methods: GET, HEAD, OPTIONS.\nWAF: Do not enable (to save costs). Alternate domain name (CNAME): Enter frontend domain (e.g., www.example.com).\nCustom SSL certificate: Select ACM certificate. Default root object: index.html. Click Create distribution.\nAfter creation, remember to update the Bucket Policy of S3 to allow OAC access (Console will suggest copying the policy).\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.2-database-storage/5.2.3-s3/","title":"Amazon S3","tags":[],"description":"","content":"Initialize Amazon S3 Buckets We need 3 buckets for different purposes.\n1. Frontend Bucket Used for static web hosting (ReactJS).\nCreate a bucket with a unique name. Uncheck Block all public access (as the web needs to be public). Enable Static website hosting. 2. Public Storage Bucket Stores auction product images (public read).\nCreate a bucket. Uncheck Block all public access. Configure Bucket Policy to allow s3:GetObject. 3. Private Storage Bucket Stores identity documents for account verification (private).\nCreate a bucket. Keep Block all public access checked. (Optional) Configure Server-side encryption. "},{"uri":"https://vdhxi.github.io/5-workshop/5.3-compute/","title":"Compute","tags":[],"description":"","content":"Initialize Amazon EC2 Amazon EC2 (Elastic Compute Cloud) will be where the backend application (Spring Boot) runs.\nContent EC2 Instance: Virtual server running the application. Security Group: Firewall controlling access. Elastic IP: Static IP address (optional, necessary if not using a Load Balancer or need a fixed outbound IP). "},{"uri":"https://vdhxi.github.io/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Performance benefits of new Amazon EC2 R8a memory-optimized instances Amazon EC2 R8a instances, powered by 5th Generation AMD EPYC processors and higher memory bandwidth, deliver significant performance improvements for memory-optimized workloads, especially MySQL databases. Based on HammerDB benchmarking, R8a demonstrates substantial gains in overall score (55% higher than R7a and 74% higher than R6a), transactions per minute (32% higher than R7a and 63% higher than R6a), and P99 latency reduction (14% lower than R7a and 25% lower than R6a). With consistent performance enabled by the 1 vCPU = 1 physical core architecture and strong scalability, R8a becomes an optimal choice for database systems and memory-intensive workloads.\nBlog 2 - Optimize latency-sensitive workloads with Amazon EC2 detailed NVMe statistics This post explains how to use Amazon EC2 detailed performance statistics for instance store NVMe volumes to monitor latency-sensitive workloads. These new metrics offer per-second granularity for queue length, IOPS, throughput, and latency histograms (including by I/O size), helping to identify performance bottlenecks and fine-tune applications. It covers how to access these statistics via nvme-cli or CloudWatch and provides scenarios for troubleshooting issues like exceeding storage limits.\nBlog 3 - How to export to Amazon S3 Tables by using AWS Step Functions Distributed Map This article demonstrates a serverless solution for automating document processing and exporting structured data to Amazon S3 Tables using AWS Step Functions Distributed Map. It details a scalable workflow that triggers on S3 uploads, uses Distributed Map for parallel processing of files (e.g., PDFs), extracts data with Amazon Textract, and streams it via Amazon Data Firehose to S3 Tables for downstream analytics with Amazon Athena.\nBlog 4 - DISA STIG for Amazon Linux 2023 is now available AWS announces the availability of the Security Technical Implementation Guide (STIG) for Amazon Linux 2023 (AL2023), developed in collaboration with DISA. This guide aids DOD and federal customers in meeting strict security compliance standards (NIST 800-53). The post also discusses how to automate the implementation of these security configurations using AWS Systems Manager and EC2 Image Builder for both existing fleets and new images.\nBlog 5 - Architecting for AI excellence: AWS launches three Well-Architected Lenses at re:Invent 2025 At re:Invent 2025, AWS introduced new and updated Well-Architected Lenses for AI: the new Responsible AI Lens, and updates to the Machine Learning (ML) Lens and Generative AI Lens. These lenses provide comprehensive best practices for ensuring AI workloads are secure, reliable, efficient, and responsible, covering the entire lifecycle from experimentation to production.\nBlog 6 - Announcing the updated AWS Well-Architected Generative AI Lens This post details the updates to the AWS Well-Architected Generative AI Lens. Key additions include guidance for Amazon SageMaker HyperPod, a new Responsible AI preamble covering eight core dimensions, a Data Architecture preamble, and an Agentic AI preamble. It also includes eight new architecture scenarios to help customers apply generative AI to common business problems effectively.\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.4-distribution/5.4.4-route53/","title":"Amazon Route 53","tags":[],"description":"","content":"Amazon Route 53 Configure DNS to point domain to CloudFront (Frontend) and ALB (Backend).\nSteps Access Route 53 -\u0026gt; Hosted zones. Select your domain. Route53 will provide 4 NS records, configure them at your domain registrar to manage the domain in Route53. This process takes a few minutes. 1. Point to Backend (ALB) Create record. Record name: api (e.g., api.example.com). Record type: A. Enable Alias. Route traffic to: Alias to Application and Classic Load Balancer. Select Region and your ALB. 2. Point to Frontend (CloudFront) Create record. Record name: www or leave blank (root domain). Record type: A. Enable Alias. Route traffic to: Alias to CloudFront distribution. Select CloudFront distribution. "},{"uri":"https://vdhxi.github.io/5-workshop/5.4-distribution/","title":"Distribution","tags":[],"description":"","content":"Content Distribution This section helps deliver the application to end users securely and with high performance.\nContent Certificate (ACM): Issue free SSL/TLS certificates. Application Load Balancer (ALB): Load balance for Backend (Spring Boot). CloudFront: CDN to distribute Frontend (ReactJS) and Static files (S3). Route 53: Manage Domain DNS. "},{"uri":"https://vdhxi.github.io/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://vdhxi.github.io/5-workshop/5.5-deploy/","title":"CI/CD","tags":[],"description":"","content":"Continuous Integration and Deployment (CI/CD) Use GitLab CI to automate the process of building and deploying the application to AWS.\nContent SSH into EC2 instance to install necessary applications. GitLab Runner: Configure runner on EC2 (or use Shared Runner). Pipeline: Define .gitlab-ci.yml file. "},{"uri":"https://vdhxi.github.io/5-workshop/","title":"System Deployment","tags":[],"description":"","content":"Deploy Auction System on AWS Welcome to the workshop on deploying an online auction system on the AWS platform. In this workshop, we will build each component of the system together based on the proposed architecture.\nObjectives Complete the installation and configuration of necessary AWS services to operate the Auction System.\nArchitecture We will follow this architecture:\nSteps Preparation: Setup VPC, IAM Role. Database \u0026amp; Storage: Configure RDS, ElastiCache, S3. Compute: Install and configure EC2. Distribution: Configure Load Balancer, CloudFront, Route 53. CI/CD: Setup automated deployment process with GitLab CI. Clean up: Delete resources after completion. "},{"uri":"https://vdhxi.github.io/5-workshop/5.6-cleanup/","title":"Clean Up","tags":[],"description":"","content":"Clean Up Resources To avoid unwanted costs after completing the workshop, please delete resources in the following order:\nDeletion Order EC2: Terminate instances. RDS \u0026amp; ElastiCache: Delete database and cache cluster. Delete Subnet Groups and Snapshots. Load Balancer \u0026amp; Target Group: Delete ALB first, then Target Group. CloudFront: Disable distribution, wait for deployment to finish, then Delete. S3: Empty bucket (delete all objects) then Delete bucket. NAT Gateway \u0026amp; Elastic IP: Delete NAT Gateway -\u0026gt; Release Elastic IP. VPC: Delete VPC (this will automatically delete Subnets, Internet Gateway, Route Tables, and related Security Groups). Note: Check the Billing Dashboard the next day to ensure there are no incurring costs.\n"},{"uri":"https://vdhxi.github.io/6-self-evaluation/","title":"Self-evaluation","tags":[],"description":"","content":"During my internship at AWS First Cloud AI Journey from September 8, 2025 to November 30, 2025, I had the opportunity to learn, practice, and apply the knowledge equipped at school into a real working environment. I participated in [brief description of project or main work], thereby improving my skills in Programming, analysis, project operation, communication.\nRegarding work style, I always try to complete tasks well, comply with regulations, and actively exchange with colleagues to improve work efficiency.\nTo objectively reflect on the internship process, I would like to self-evaluate based on the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional Knowledge and Skills Industry understanding, applying knowledge to practice, tool usage skills, work quality ✅ ☐ ☐ 2 Learning Ability Absorbing new knowledge, learning quickly ☐ ✅ ☐ 3 Proactiveness Self-learning, accepting tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of Responsibility Completing work on time, ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to time, regulations, work processes ✅ ☐ ☐ 6 Progressiveness Willing to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas, reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues, participating in groups ☐ ✅ ☐ 9 Professional Conduct Respecting colleagues, partners, work environment ✅ ☐ ☐ 10 Problem Solving Mindset Identifying problems, proposing solutions, creativity ✅ ☐ ☐ 11 Contribution to Project/Organization Work efficiency, improvement initiatives, recognition from team ✅ ☐ ☐ 12 Overall General assessment of the entire internship process ✅ ☐ ☐ Needs Improvement Learn to communicate better in daily communication and at work, handling situations "},{"uri":"https://vdhxi.github.io/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here you can freely contribute your personal opinions about your experiences participating in the First Cloud Journey program, helping the FCJ team improve shortcomings based on the following categories:\nGeneral Evaluation 1. Work Environment\nDynamic and positive working environment. Members are always ready to help and share new knowledge with each other for mutual development. The workspace is neat and comfortable; the atmosphere is very professional yet also very comfortable, helping to increase work efficiency.\n2. Support from mentor / team admin\nThe mentor provided very detailed guidance, explained clearly when I didn\u0026rsquo;t understand, and always encouraged me to ask questions. The team admin supported procedures and documents, creating conditions for me to work favorably.\n3. Relevance between work and major\nThe work I was assigned matched the knowledge I learned at school, while also expanding into new areas that I had never approached before. Thanks to this, I was able to implement projects that I had cherished for a long time but was still struggling with where to start. Besides that, I both consolidated my foundational knowledge and learned additional practical skills. Working at the company was truly a stroke of luck.\n4. Learning opportunities \u0026amp; skill development\nDuring the internship, I learned many new skills such as approaching and using new technologies, using project management tools, working processes in a more professional manner, and also professional communication in a corporate environment. The mentor also shared a lot of practical experience helping me orient my career better.\n5. Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone interacts and supports each other, working seriously but still having fun. However, there were still a few peers with bad attitudes, lacking the spirit of actively learning new knowledge, not performing assigned tasks well, and having an unserious attitude towards the assigned project.\n6. Policies / Benefits for interns\nThe company organizes events and workshops to share experience about new knowledge, which is a very wonderful thing.\nOther Questions What were you most satisfied with during the internship? What do you think the company needs to improve for future interns? If introducing to friends, would you recommend them to intern here? Why? Suggestions \u0026amp; Wishes Do you have any suggestions to improve the internship experience? Do you want to continue this program in the future? Other feedback (freely share): "},{"uri":"https://vdhxi.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://vdhxi.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]